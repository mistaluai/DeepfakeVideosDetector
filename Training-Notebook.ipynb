{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1682720149d17895","cell_type":"markdown","source":"## Imports","metadata":{}},{"id":"9e8f0294-5537-4444-97ca-cabc5ce59782","cell_type":"code","source":"!pip install pyav","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"initial_id","cell_type":"code","source":"import itertools\nimport torch\nimport os\nimport random\nimport numpy as np\nimport torchvision\nfrom torch.utils.data import DataLoader, IterableDataset, get_worker_info\nfrom torchvision.datasets.folder import make_dataset\nfrom torchvision import transforms as t\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models import ResNet50_Weights\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torchvision.transforms import v2\nfrom torch.jit import script, trace, trace_module\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fba047dd9e84623c","cell_type":"markdown","source":"## Utils Class","metadata":{}},{"id":"d7d68a20119bc3f5","cell_type":"code","source":"class TrainingUtilities:\n    @staticmethod\n    def save_model(model, model_descriptor, save_folder, verbose=0):\n        torch.save(model.state_dict(), save_folder + f\"/{model_descriptor}b1_model.pth\")\n        if verbose > 0:\n            print(f\"Saved model to {save_folder}/b1_model.pth\")\n\n    @staticmethod\n    def save_checkpoint(epoch, model_state_dict, optimizer_state_dict, scheduler_state_dict=None, save_folder='', verbose=0):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model_state_dict,\n            'optimizer_state_dict': optimizer_state_dict,\n            'scheduler_state_dict': scheduler_state_dict\n        }\n        torch.save(checkpoint, save_folder + f'/checkpoint-epoch{epoch}.pth')\n        if verbose > 0:\n            print(f'Saved checkpoint to {save_folder}/checkpoint-epoch{epoch}.pth')\n\n    @staticmethod\n    def load_checkpoint(model, optimizer, checkpoint_path, scheduled, verbose=0):\n        checkpoint = torch.load(checkpoint_path)\n\n        if verbose > 0:\n            print(f\"Loading checkpoint from {checkpoint_path}\")\n\n        epoch = checkpoint['epoch']\n        model_state_dict = checkpoint['model_state_dict']\n        optimizer_state_dict = checkpoint['optimizer_state_dict']\n        scheduler_state_dict = checkpoint['scheduler_state_dict']\n        model = model.load_state_dict(model_state_dict)\n        if scheduled:\n            optimizer.load_state_dict(optimizer_state_dict, scheduler_state_dict)\n        else:\n            optimizer = optimizer.load_state_dict(optimizer_state_dict)\n        return epoch, model, optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f89195a1d602111c","cell_type":"markdown","source":"## Dataset","metadata":{}},{"id":"dc5603fc17f4643a","cell_type":"code","source":"def get_samples(root, extensions=(\".mp4\", \".avi\")):\n    samples = []\n\n    # Define class labels\n    class_to_idx = {\n        \"DFD_original sequences\": 0,  # Real videos\n        \"DFD_manipulated_sequences\": 1  # Deepfake videos\n    }\n\n    for class_name, label in class_to_idx.items():\n        class_dir = os.path.join(root, class_name)\n        if class_name == 'DFD_manipulated_sequences':\n            class_dir = os.path.join(class_dir, class_name)\n        print(class_dir)\n        if not os.path.exists(class_dir):\n            continue\n\n        # Get all video files in the directory\n        for filename in os.listdir(class_dir):\n            if filename.endswith(extensions):\n                file_path = os.path.join(class_dir, filename)\n                samples.append((file_path, label))\n\n    return samples\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness. NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n  print(f'Random seed {seed} has been set.')\n\ndef get_datasets(root, splits, epoch_size=None, frame_transform=None, video_transform=None, clip_len=16, seed=2024):\n    train_split = splits[0]\n    val_split = splits[1]\n    test_split = splits[2]\n\n    samples = get_samples(root)\n    print(samples[0])\n    print(samples[-1])\n    \n    set_seed(seed, seed_torch=True)\n    random.shuffle(samples)\n\n    start, end = train_split\n    train_samples = samples[start:end]\n    start, end = val_split\n    val_samples = samples[start:end]\n    start, end = test_split\n    test_samples = samples[start:end]\n\n    train_dataset = VideosDataset(train_samples, frame_transform=frame_transform, video_transform=video_transform, clip_len=clip_len)\n    val_dataset = VideosDataset(val_samples, frame_transform=frame_transform, video_transform=video_transform, clip_len=clip_len)\n    test_dataset = VideosDataset(test_samples, frame_transform=frame_transform, video_transform=video_transform, clip_len=clip_len)\n\n    return train_dataset, val_dataset, test_dataset\n\n\n\nclass VideosDataset(torch.utils.data.IterableDataset):\n    def __init__(self, samples, epoch_size=None, frame_transform=None, video_transform=None, clip_len=16):\n        super(VideosDataset).__init__()\n        self.samples = samples\n\n        # Allow for temporal jittering\n        if epoch_size is None:\n            epoch_size = len(self.samples)\n        self.epoch_size = epoch_size\n\n        self.clip_len = clip_len\n        if frame_transform is None:\n            self.frame_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Resize(255),\n                                 v2.CenterCrop(224)])\n        else:\n            self.frame_transform = frame_transform\n\n        self.video_transform = video_transform\n\n    def __len__(self):\n        return self.epoch_size\n\n    def __pad_video(self, video_frames):\n        \"\"\"Prepad video frames to match clip length.\"\"\"\n        n = len(video_frames)\n        if n == self.clip_len:\n            return video_frames\n\n        # Create zero frames\n        pad_tensor = torch.zeros_like(video_frames[0])\n        pad_frames = [pad_tensor] * (self.clip_len - n)  # List of zero frames\n\n        return pad_frames + video_frames  # Prepadding at the beginning\n\n    def __iter__(self):\n        for i in range(self.epoch_size):\n            # Get random sample\n            path, target = random.choice(self.samples)\n            # Get video object\n            vid = torchvision.io.VideoReader(path, \"video\")\n            metadata = vid.get_metadata()\n            video_frames = []  # video frame buffer\n\n            for frame in itertools.islice(vid, self.clip_len):\n                video_frames.append(self.frame_transform(frame['data']))\n            video_frames = self.__pad_video(video_frames)\n            # Stack it into a tensor\n            video = torch.stack(video_frames, 0)\n            if self.video_transform:\n                video = self.video_transform(video)\n            output = {\n                'video': video,\n                'target': target\n            }\n            yield output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b23f5f6fac643342","cell_type":"markdown","source":"## Model","metadata":{}},{"id":"b32cae31ec132ef8","cell_type":"code","source":"class CNN_LSTM(nn.Module):\n    def __init__(self, hidden_dim=512, num_layers=3, num_classes=2):\n        super(CNN_LSTM, self).__init__()\n        cnn = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        self.feature_extractor = nn.Sequential(*list(cnn.children())[:-1])\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling to (B, 2048, 1, 1)\n\n        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(1024, num_classes),\n        )\n\n    def forward(self, x):\n        batch_size, frames, C, H, W = x.shape  # (Batch, Frames, C, H, W)\n\n        x = x.view(batch_size * frames, C, H, W)  # (Batch × Frames, C, H, W)\n        features = self.feature_extractor(x)\n        features = self.pool(features).squeeze(-1).squeeze(-1)  # (Batch × Frames, 2048)\n\n        features = features.view(batch_size, frames, -1)  # (Batch, Frames, Feature_Dim)\n        lstm_out, _ = self.lstm(features)\n        out = self.fc(lstm_out[:, -1, :])\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"68e81fea4f369935","cell_type":"markdown","source":"## Trainer","metadata":{}},{"id":"e2a22955582fdd13","cell_type":"code","source":"class ModelTrainer:\n    def __init__(self, model, optimizer, scheduled, criterion, epochs, dataloaders, device, save_folder,\n                 is_continue=False, checkpoint=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduled = scheduled\n        self.criterion = criterion\n        self.epochs = epochs\n        self.dataloaders = dataloaders\n        self.DEVICE = device\n        self.save_folder = save_folder\n        self.is_continue = is_continue\n        self.checkpoint = checkpoint\n        self.scaler = torch.amp.GradScaler()\n\n    def train_model(self, verbose=0):\n        model, optimizer, criterion, epochs, dataloaders = self.model, self.optimizer, self.criterion, self.epochs, self.dataloaders\n        device = self.DEVICE\n        scaler = self.scaler\n        training_epoch = 0\n        epoch = 0\n        if self.is_continue:\n\n            if verbose > 0:\n                print(f\"Continuing from checkpoint {self.checkpoint}\")\n\n            epoch, model, optimizer = TrainingUtilities.load_checkpoint(model, optimizer, self.checkpoint, self.scheduled, verbose)\n\n        for training_epoch in range(epoch, epochs):\n            print(f\"\\nTraining epoch {training_epoch}\")\n            train_losses = []\n            val_losses = []\n            val_accuracies = []\n            avg_train_loss = 0\n            avg_val_loss = 0\n            val_accuracy = 0\n            train_accuracy = 0\n            train_time = 0\n            val_time = 0\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    start_time = time.time()\n                    print(\"Training phase.....\")\n                    train_loader = tqdm(dataloaders['train'], desc='train')\n                    model.train()\n                    train_loss = 0\n                    correct_train = 0\n                    total_train = 0\n                    for batch in train_loader:\n                        video = batch['video'].to(device)  # (Batch, Frames, C, H, W)\n                        target = batch['target'].to(device)\n\n                        optimizer.zero_grad()\n                        with torch.amp.autocast('cuda'):\n                            outputs = model(video)\n                            loss = criterion(outputs, target)\n\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        scaler.update()\n\n                        train_loss += loss.item()\n                        _, predicted = torch.max(outputs, 1)\n                        # print(f'predicted is {predicted}, target is {target}')\n                        correct_train += (predicted == target).sum().item()\n                        total_train += target.size(0)\n\n                    avg_train_loss = train_loss / len(train_loader)\n                    train_accuracy = correct_train / total_train\n                    end_time = time.time()\n                    train_time = end_time - start_time\n                    formatted_time = time.strftime(\"%Mmins %Ssecs\", time.gmtime(train_time))\n                    print(f\"Training completed in {formatted_time}\")\n                else:\n                    print(\"Validation phase.....\")\n                    val_loader = tqdm(dataloaders['val'], desc='val')\n                    start_time = time.time()\n                    avg_val_loss, val_accuracy = self.evaluate(val_loader, training_epoch)\n                    end_time = time.time()\n                    val_time = end_time - start_time\n                    formatted_time = time.strftime(\"%Mmins %Ssecs\", time.gmtime(val_time))\n                    print(f\"Validation completed in {formatted_time}\")\n\n            print(\n                f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")        #Test model\n        self.test(tqdm(dataloaders['test'], desc='test'))\n\n    def test(self, test_loader):\n        model, criterion,  device = self.model, self.criterion, self.DEVICE\n        model.eval()\n        test_loss = 0\n\n        correct_test = 0\n        total_test = 0\n\n        with torch.no_grad():\n            with torch.amp.autocast('cuda'):\n                for batch in test_loader:\n                    video = batch['video'].to(device)\n                    target = batch['target'].to(device)\n\n                    outputs = model(video)\n                    loss = criterion(outputs, target)\n                    test_loss += loss.item()\n\n                    _, predicted = torch.max(outputs, 1)\n                    correct_test += (predicted == target).sum().item()\n                    total_test += target.size(0)\n\n        avg_test_loss = test_loss / len(test_loader)\n        test_accuracy = correct_test / total_test\n        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n        TrainingUtilities.save_model(model,'final_', self.save_folder)\n\n    def evaluate(self,val_loader,epoch, verbose=0):\n        model, criterion, device = self.model, self.criterion, self.DEVICE\n        model.eval()\n        val_loss = 0\n        correct_val = 0\n        total_val = 0\n        with torch.no_grad():\n            with torch.amp.autocast('cuda'):\n                for batch in val_loader:\n                    video = batch['video'].to(device)\n                    target = batch['target'].to(device)\n\n                    outputs = model(video)\n                    loss = criterion(outputs, target)\n                    val_loss += loss.item()\n\n                    _, predicted = torch.max(outputs, 1)\n                    correct_val += (predicted == target).sum().item()\n                    total_val += target.size(0)\n        val_accuracy = correct_val / total_val\n        avg_val_loss = val_loss / len(val_loader)\n        return avg_val_loss, val_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4cb1d6f17b97abee","cell_type":"markdown","source":"## Code","metadata":{}},{"id":"18c08bc3a099cde1","cell_type":"code","source":"dataset_root = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset\"\ndataset_size = len(get_samples(dataset_root))\ndataset_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bf76fdec60f486b8","cell_type":"code","source":"# Define split sizes\ntrain_size = int(0.7 * dataset_size)  # 70% for training\nval_size = int(0.15 * dataset_size)   # 15% for validation\ntest_size = dataset_size - train_size - val_size  # 15% for testing\n\nprint(f'train: {train_size}, test: {val_size}, val: {test_size}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f868eecd-20d4-467f-97ea-ff4995d15988","cell_type":"code","source":"# work on small sample \nval_size = int(val_size/3)\ntrain_size = int(train_size/3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f39207ac685e375f","cell_type":"code","source":"# Split dataset\ntrain_split = (0,train_size)\nval_split = (train_size, val_size + train_size)\ntest_split = (val_size + train_size, val_size + train_size + test_size)\n\nsplits = [train_split, val_split, test_split]\nframes=8\ntrain_dataset, val_dataset, test_dataset = get_datasets(root=dataset_root,splits=splits, clip_len=frames)\n# train_dataset = RandomDataset(root=dataset_root, clip_len=frames, split=train_split)\n# val_dataset = RandomDataset(root=dataset_root, clip_len=frames, split=val_split)\n# test_dataset = RandomDataset(root=dataset_root, clip_len=frames, split=test_split)\n\nbatch_size = 32\nprint(f'train: {train_dataset.__len__()}, test: {val_dataset.__len__()}, val: {test_dataset.__len__()}')# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size,drop_last=True)\n\ndataloaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"99a0d8996dbcc905","cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1f8d64d1-2746-46ff-895d-aaeb7959827f","cell_type":"code","source":"example_input = {\n    'batch_size':batch_size,\n    'frames':frames,\n    'c':3,\n    'H':224,\n    'W':224\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3cc32fab4f5e7a4f","cell_type":"code","source":"# Initialize model, loss, and optimizer\nmodel = CNN_LSTM().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"da48c532338312d0","cell_type":"code","source":"save_folder = '/kaggle/working/'\ntrainer = ModelTrainer(model=model, optimizer=optimizer, criterion=criterion, epochs=10, dataloaders=dataloaders, device=device, save_folder=save_folder, scheduled=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"578226dc81f93abf","cell_type":"code","source":"trainer.train_model(verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}