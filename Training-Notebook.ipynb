{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "1682720149d17895"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import itertools\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.folder import make_dataset\n",
    "from torchvision import transforms as t\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils Class",
   "id": "fba047dd9e84623c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TrainingUtilities:\n",
    "    @staticmethod\n",
    "    def save_model(model, model_descriptor, save_folder, verbose=0):\n",
    "        torch.save(model.state_dict(), save_folder + f\"/{model_descriptor}b1_model.pth\")\n",
    "        if verbose > 0:\n",
    "            print(f\"Saved model to {save_folder}/b1_model.pth\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_checkpoint(epoch, model_state_dict, optimizer_state_dict, scheduler_state_dict=None, save_folder='', verbose=0):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'scheduler_state_dict': scheduler_state_dict\n",
    "        }\n",
    "        torch.save(checkpoint, save_folder + f'/checkpoint-epoch{epoch}.pth')\n",
    "        if verbose > 0:\n",
    "            print(f'Saved checkpoint to {save_folder}/checkpoint-epoch{epoch}.pth')\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(model, optimizer, checkpoint_path, scheduled, verbose=0):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "\n",
    "        epoch = checkpoint['epoch']\n",
    "        model_state_dict = checkpoint['model_state_dict']\n",
    "        optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "        scheduler_state_dict = checkpoint['scheduler_state_dict']\n",
    "        model = model.load_state_dict(model_state_dict)\n",
    "        if scheduled:\n",
    "            optimizer.load_state_dict(optimizer_state_dict, scheduler_state_dict)\n",
    "        else:\n",
    "            optimizer = optimizer.load_state_dict(optimizer_state_dict)\n",
    "        return epoch, model, optimizer"
   ],
   "id": "d7d68a20119bc3f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "f89195a1d602111c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def get_samples(root, extensions=(\".mp4\", \".avi\")):\n",
    "    samples = []\n",
    "\n",
    "    # Define class labels\n",
    "    class_to_idx = {\n",
    "        \"DFD_original_sequences\": 0,  # Real videos\n",
    "        \"DFD_manipulated_sequences\": 1  # Deepfake videos\n",
    "    }\n",
    "\n",
    "    for class_name, label in class_to_idx.items():\n",
    "        class_dir = os.path.join(root, class_name)\n",
    "        if class_name == 'DFD_manipulated_sequences':\n",
    "            class_dir = os.path.join(class_dir, class_name)\n",
    "        print(class_dir)\n",
    "        if not os.path.exists(class_dir):\n",
    "            continue\n",
    "\n",
    "        # Get all video files in the directory\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith(extensions):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                samples.append((file_path, label))\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "class RandomDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, root, epoch_size=None, frame_transform=None, video_transform=None, clip_len=16):\n",
    "        super(RandomDataset).__init__()\n",
    "\n",
    "        self.samples = get_samples(root)\n",
    "\n",
    "        # Allow for temporal jittering\n",
    "        if epoch_size is None:\n",
    "            epoch_size = len(self.samples)\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_transform = frame_transform\n",
    "        self.video_transform = video_transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.epoch_size):\n",
    "            # Get random sample\n",
    "            path, target = random.choice(self.samples)\n",
    "            # Get video object\n",
    "            vid = torchvision.io.VideoReader(path, \"video\")\n",
    "            metadata = vid.get_metadata()\n",
    "            video_frames = []  # video frame buffer\n",
    "\n",
    "            # Seek and return frames\n",
    "            max_seek = metadata[\"video\"]['duration'][0] - (self.clip_len / metadata[\"video\"]['fps'][0])\n",
    "            start = random.uniform(0., max_seek)\n",
    "            for frame in itertools.islice(vid.seek(start), self.clip_len):\n",
    "                video_frames.append(self.frame_transform(frame['data']))\n",
    "                current_pts = frame['pts']\n",
    "            # Stack it into a tensor\n",
    "            video = torch.stack(video_frames, 0)\n",
    "            if self.video_transform:\n",
    "                video = self.video_transform(video)\n",
    "            output = {\n",
    "                'video': video,\n",
    "                'target': target\n",
    "            }\n",
    "            yield output"
   ],
   "id": "dc5603fc17f4643a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "b23f5f6fac643342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_layers=3, num_classes=2):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        cnn = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.feature_extractor = nn.Sequential(*list(cnn.children())[:-1])\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling to (B, 2048, 1, 1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, frames, C, H, W = x.shape  # (Batch, Frames, C, H, W)\n",
    "\n",
    "        x = x.view(batch_size * frames, C, H, W)  # (Batch × Frames, C, H, W)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = self.pool(features).squeeze(-1).squeeze(-1)  # (Batch × Frames, 2048)\n",
    "\n",
    "        features = features.view(batch_size, frames, -1)  # (Batch, Frames, Feature_Dim)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ],
   "id": "b32cae31ec132ef8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trainer",
   "id": "68e81fea4f369935"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, optimizer, scheduled, criterion, epochs, dataloaders, device, save_folder,\n",
    "                 is_continue=False, checkpoint=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduled = scheduled\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        self.DEVICE = device\n",
    "        self.save_folder = save_folder\n",
    "        self.is_continue = is_continue\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def train_model(self, verbose=0):\n",
    "        model, optimizer, criterion, epochs, dataloaders = self.model, self.optimizer, self.criterion, self.epochs, self.dataloaders\n",
    "        device = self.DEVICE\n",
    "\n",
    "        training_epoch = 0\n",
    "        epoch = 0\n",
    "        if self.is_continue:\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"Continuing from checkpoint {self.checkpoint}\")\n",
    "\n",
    "            epoch, model, optimizer = TrainingUtilities.load_checkpoint(model, optimizer, self.checkpoint, self.scheduled, verbose)\n",
    "\n",
    "        for training_epoch in range(epoch, epochs):\n",
    "\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            avg_train_loss = 0\n",
    "            avg_val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            train_accuracy = 0\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    train_loader = dataloaders['train']\n",
    "                    model.train()\n",
    "                    train_loss = 0\n",
    "                    correct_train = 0\n",
    "                    total_train = 0\n",
    "                    for batch in tqdm(train_loader, desc=f\"Epoch {training_epoch + 1}/5 - Training\"):\n",
    "                        video = batch['video'].to(device)  # (Batch, Frames, C, H, W)\n",
    "                        target = batch['target'].to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(video)\n",
    "                        loss = criterion(outputs, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        train_loss += loss.item()\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        correct_train += (predicted == target).sum().item()\n",
    "                        total_train += target.size(0)\n",
    "\n",
    "                    avg_train_loss = train_loss / len(train_loader)\n",
    "                    train_accuracy = correct_train / total_train\n",
    "                else:\n",
    "                    val_loader = dataloaders['val']\n",
    "                    avg_val_loss, val_accuracy = self.evaluate(val_loader, training_epoch)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/5], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")        #Test model\n",
    "        self.test(dataloaders['test'])\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        model, criterion,  device = self.model, self.criterion, self.DEVICE\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                video = batch['video'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "\n",
    "                outputs = model(video)\n",
    "                loss = criterion(outputs, target)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_test += (predicted == target).sum().item()\n",
    "                total_test += target.size(0)\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = correct_test / total_test\n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        TrainingUtilities.save_model(model,'final_', self.save_folder)\n",
    "\n",
    "    def evaluate(self,val_loader,epoch, verbose=0):\n",
    "        model, criterion, device = self.model, self.criterion, self.DEVICE\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/5 - Validation\"):\n",
    "                video = batch['video'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "\n",
    "                outputs = model(video)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "                total_val += target.size(0)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        return avg_val_loss, val_accuracy\n"
   ],
   "id": "e2a22955582fdd13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Code",
   "id": "4cb1d6f17b97abee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset_root = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset\"\n",
    "dataset = RandomDataset(root=dataset_root, clip_len=16)"
   ],
   "id": "18c08bc3a099cde1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define split sizes\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "val_size = int(0.15 * len(dataset))   # 15% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # 15% for testing"
   ],
   "id": "bf76fdec60f486b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}"
   ],
   "id": "f39207ac685e375f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "99a0d8996dbcc905"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = CNN_LSTM().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "3cc32fab4f5e7a4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_folder = '/kaggle/working/'\n",
    "trainer = ModelTrainer(model=model, optimizer=optimizer, criterion=criterion, epochs=10, dataloaders=dataloaders, device=device, save_folder=save_folder, scheduled=False)"
   ],
   "id": "da48c532338312d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train_model(verbose=1)",
   "id": "578226dc81f93abf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
